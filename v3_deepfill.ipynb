{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v3_deepfill.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItalianPepper/deepfill/blob/master/v3_deepfill.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRO-slZmMvvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clonig repo datawig - i dataset sono dentro la cartella examples\n",
        "!git clone \"https://github.com/awslabs/datawig\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M387zRnyOqfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Necessario per far funzionare datawig\n",
        "!pip install mxnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwtY_v-4sbHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cartella per i risultati della soluzione\n",
        "!mkdir result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcDneh3fiUZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cartella dei risultati di datawig\n",
        "!mkdir result_datawig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-fRWgORNkl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r result\n",
        "!rm -r result_datawig\n",
        "!rm -r imputer_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Fd-3xHeVc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Zip pronto per il download dei risultati\n",
        "!zip -r \"/content/result.zip\" \"/content/result\"\n",
        "!zip -r \"/content/result_datawig.zip\" \"/content/result_datawig\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pYIxFuY7-iZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collegamento con Drive (nel caso si voglia caricare dataset da drive o salvare i risultati)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WCeGA10_us05",
        "colab": {}
      },
      "source": [
        "# Il valore di y viene aggiunto al tokenizer ma non viene tokenizzato\n",
        "# uso di keras e non di tensorflow.keras -> manca l'engine utile alla costurzione dell'attention layer\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import plot_model\n",
        "from keras import initializers as initializers, regularizers, constraints\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "import re\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import nltk\n",
        "\n",
        "if not os.path.exists(\"./root/ntlk_data/corpora/stopwords\"):\n",
        "  nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def random_split(data_frame: pd.DataFrame,\n",
        "                 split_ratios: List[float] = None,\n",
        "                 seed: int = 10) -> List[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Da datawig.utils\n",
        "\n",
        "    Shuffles and splits a Data frame into partitions with specified percentages of data\n",
        "\n",
        "    :param data_frame: a pandas DataFrame\n",
        "    :param split_ratios: percentages of splits\n",
        "    :param seed: seed of random number generator\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if split_ratios is None:\n",
        "        split_ratios = [.8, .2]\n",
        "    sections = np.array([int(r * len(data_frame)) for r in split_ratios]).cumsum()\n",
        "    return np.split(data_frame.sample(frac=1, random_state=seed), sections)[:len(split_ratios)]\n",
        "\n",
        "\n",
        "\n",
        "def clean_df(df, other_unclassified_values = []):\n",
        "    \n",
        "    # Rimozione dei caratteri speciali con la stringa vuota\n",
        "    df = df.replace(\n",
        "        to_replace=r'[!@#$%^&*()_+\\-=\\[\\]{};\\':\\\"\\\\|,.<>\\/?]',\n",
        "               value=\"\", regex=True)\n",
        "    \n",
        "    # Cambio di valori come ad esempio 'None', 'Unclassified', ecc... a NaN\n",
        "    if len(other_unclassified_values) > 0:\n",
        "      df = df.replace(to_replace=other_unclassified_values, value=\"\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def is_y_categorical (df, y_label):\n",
        "\n",
        "    y_col = df[y_label].tolist()\n",
        "\n",
        "    categories = set(y for y in y_col)\n",
        "    \n",
        "    if len(categories) == len(y_col):\n",
        "        raise Exception(\"Y is not categorical!\")\n",
        "\n",
        "\n",
        "def fit_tokenizer(df, input_x_label, output_y_label, size_dict_words, other_unclassified_values=[]):\n",
        "\n",
        "    all_attributes = input_x_label + [output_y_label]\n",
        "    words = []\n",
        "\n",
        "    df = df.replace(to_replace=r'[!@#$%^&*()_+\\-=\\[\\]{};\\':\\\"\\\\|,.<>\\/?]',\n",
        "               value=\"\", regex=True)\n",
        "\n",
        "    if len(other_unclassified_values) > 0:\n",
        "      df = df.replace(to_replace=other_unclassified_values, value=\"\")\n",
        "\n",
        "    for i in range(0, len(df)):\n",
        "      \n",
        "      for attribute in all_attributes:\n",
        "          row = df.at[i, attribute]\n",
        "          \n",
        "          if len(row.split()) > 1:\n",
        "\n",
        "            for x in row.split():\n",
        "              words.append([x])\n",
        "          else:\n",
        "              words.append([row])\n",
        "\n",
        "    # default param\n",
        "    # Il tokenizer considera solo le parole più frequenti con param num_words = -1\n",
        "    tokenizer = Tokenizer(num_words = size_dict_words,\n",
        "                          filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                          lower = True,\n",
        "                          split=\" \",\n",
        "                          char_level=False)\n",
        "    \n",
        "    tokenizer.fit_on_texts(words)\n",
        "    \n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def tokenizing_x_y(df, input_x_label, output_y_label, tokenizer):\n",
        "\n",
        "    tokenized_x = []\n",
        "    tokenized_y = []\n",
        "\n",
        "    mean_length_row = 0\n",
        "\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "\n",
        "    all_attributes = input_x_label + [output_y_label]\n",
        "\n",
        "    for i in range(0, len(df)):\n",
        "        row_x = []\n",
        "        row_y = []\n",
        "        \n",
        "        for attribute in all_attributes:\n",
        "            # Può essere una singola parola o una frase\n",
        "            element = df.at[i, attribute]\n",
        "            element = element.lower()\n",
        "\n",
        "            if attribute in input_x_label:\n",
        "\n",
        "                if len(element.split()) > 1:\n",
        "                    tokenized_row = []\n",
        "\n",
        "                    for word in element.split():\n",
        "\n",
        "                        # Le stop words sono eliminate sono sul lato degli attributi di X\n",
        "                        if word not in stop_words and word in tokenizer.word_index:\n",
        "\n",
        "                          emb_word = tokenizer.word_index[word]\n",
        "                          tokenized_row.append(emb_word)\n",
        "\n",
        "                    row_x.extend(tokenized_row)\n",
        "\n",
        "                else:\n",
        "                  if len(element.split()) == 1 and element != \" \" and element in tokenizer.word_index and element not in stop_words:\n",
        "                    \n",
        "                    tokenized_single = tokenizer.word_index[element]\n",
        "                        \n",
        "                    row_x.append(tokenized_single)\n",
        "            else:\n",
        "              row_y.append(element)\n",
        "        \n",
        "        if len(row_x) > 0:\n",
        "          \n",
        "          tokenized_x.append(row_x)\n",
        "          tokenized_y.append(row_y)\n",
        "\n",
        "    return tokenized_x, tokenized_y\n",
        "\n",
        "\n",
        "def labeling_y(df, y_label, y_dataset):\n",
        "\n",
        "    values = df[y_label].tolist()\n",
        "    values_set = set(y for y in values)\n",
        "    \n",
        "    mapping_y = {y:i for i, y in enumerate(values_set)}\n",
        "    \n",
        "    mapped_y = []\n",
        "\n",
        "    for i in range(0, len(y_dataset)):\n",
        "        map_y = mapping_y.get(y_dataset[i][0])\n",
        "        mapped_y.append(map_y)\n",
        "\n",
        "    return mapped_y, mapping_y\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n",
        "    - Yang et. al.\n",
        "    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
        "    Theano backend\n",
        "    \"\"\"\n",
        "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
        "        # Initializer \n",
        "        self.supports_masking = True\n",
        "        self.return_coefficients = return_coefficients\n",
        "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
        "        self.attention_dim = attention_dim\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Builds all weights\n",
        "        # W = Weight matrix, b = bias vector, u = context vector\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
        "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
        "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
        "        self.trainable_weights = [self.W, self.b, self.u]\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, hit, mask=None):\n",
        "        # Here, the actual calculation is done\n",
        "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
        "        uit = K.tanh(uit)\n",
        "        \n",
        "        ait = K.dot(uit, self.u)\n",
        "        ait = K.squeeze(ait, -1)\n",
        "        ait = K.exp(ait)\n",
        "        \n",
        "        if mask is not None:\n",
        "            ait *= K.cast(mask, K.floatx())\n",
        "\n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = hit * ait\n",
        "        \n",
        "        if self.return_coefficients:\n",
        "            return [K.sum(weighted_input, axis=1), ait]\n",
        "        else:\n",
        "            return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_coefficients:\n",
        "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
        "        else:\n",
        "            return input_shape[0], input_shape[-1]\n",
        "\n",
        "\n",
        "def create_model(size_dict_words, n_features_word, max_length_row, out_classes, embedding_maxtrix = None):\n",
        "    \n",
        "    # Usare l'attention layer:\n",
        "    # https://github.com/adsieg/Multi_Text_Classification/blob/master/%5BSupervised%5D%20%5BDL%20method%5D%20GRU_HAN.ipynb\n",
        "\n",
        "    # Viene passata una parola per volta\n",
        "    layer_conv_params = [[100, 3, 3], [50, 5, 5]]\n",
        "\n",
        "    in_layer = Input(shape=(max_length_row, ))\n",
        "    \n",
        "    x =  Embedding(size_dict_words, n_features_word, \n",
        "                                 input_length = max_length_row)(in_layer)\n",
        "\n",
        "    x = Dropout(0.2)(x)\n",
        "    \n",
        "    for filters, kernel_size, pool_size  in layer_conv_params:\n",
        "      \n",
        "      x = Conv1D(filters = filters, kernel_size = kernel_size, activation=\"relu\")(x)\n",
        "\n",
        "      x = BatchNormalization()(x)\n",
        "      x = SpatialDropout1D(0.1)(x)\n",
        "      x = MaxPooling1D(pool_size = pool_size)(x)\n",
        "    \n",
        "    x = Bidirectional(GRU(50, return_sequences=True))(x)\n",
        "    x = AttentionLayer(n_features_word, False)(x)\n",
        "\n",
        "    out = Dense(out_classes, activation=\"softmax\")(x)\n",
        "    \n",
        "    model = Model(in_layer, out)\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", \n",
        "                  metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def train_classifier(x, y, max_length_row, n_features_word,\n",
        "                     out_classes, size_dict_words, embedding_matrix=None):\n",
        "    \n",
        "    n_epochs = 3\n",
        "    batch_size = 32\n",
        "\n",
        "    model = create_model(size_dict_words, n_features_word,\n",
        "                         max_length_row, out_classes)\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(x, y,\n",
        "                        validation_split=0.20,\n",
        "                        epochs = n_epochs,\n",
        "                        batch_size = batch_size,\n",
        "                        verbose = 2)\n",
        "    \n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.plot(history.history[\"loss\"])\n",
        "    plt.plot(history.history[\"val_loss\"])\n",
        "    plt.title(\"Learning Curves\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "\n",
        "    plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
        "    \n",
        "    name = \"./result/plot_train_val.png\"\n",
        "    plt.savefig(name)\n",
        "    plt.close()\n",
        "    \n",
        "    plot_model(model, to_file=\"./result/model_plot.png\", show_shapes=True, show_layer_names=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "def plot_dataset(df, y_label):\n",
        "    \n",
        "    values = df[y_label].tolist()\n",
        "    categories = set(y for y in values)\n",
        "    counter_y_dataset = {y:values.count(y) for y in categories}\n",
        "     \n",
        "    n_classes = len(counter_y_dataset.keys())\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.bar(np.arange(0, n_classes), height=counter_y_dataset.values())\n",
        "    plt.title(\"Dataset Classes Distribution\")\n",
        "    plt.xticks(np.arange(0, n_classes, step=1.0), labels=counter_y_dataset.keys(), rotation=\"vertical\")\n",
        "    plt.savefig(\"./result/dataset_plot.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_testing(y_true, y_pred, mapping_y):\n",
        "    # Plotting dataset\n",
        "\n",
        "    counter_y_true = dict()\n",
        "    \n",
        "    for k in mapping_y.keys():\n",
        "      counter_y_true.update({k:y_true.count(k)}) \n",
        "        \n",
        "    counter_y_pred = dict()\n",
        "\n",
        "    for k in mapping_y.keys():\n",
        "      counter_y_pred.update({k:y_pred.count(k)})\n",
        "\n",
        "    n_classes = len(mapping_y.keys())\n",
        "\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.bar(np.arange(0, n_classes), color=\"red\", alpha=0.5, label=\"y_true\", height=counter_y_true.values())\n",
        "    plt.bar(np.arange(0, n_classes), color=\"yellow\", alpha=0.5, label=\"y_pred\", height=counter_y_pred.values())\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.title(\"True vs Pred\")\n",
        "    plt.xticks(np.arange(0, n_classes, step=1.0), labels=mapping_y.keys(), rotation=\"vertical\")\n",
        "    plt.savefig(\"./result/hist_all_together.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def imputing_missing_values(df_test, input_x_label, output_y_label, tokenizer,\n",
        "                            mapping_y, max_length_row, n_features_word, model,\n",
        "                            other_unclassified_values=[], dir_out=\"./result/\"):\n",
        "    \n",
        "    df_test = clean_df(df_test, other_unclassified_values=[])\n",
        "\n",
        "    tokenized_x = []\n",
        "    \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for i in range(0, len(df_test)):\n",
        "        row_x = []\n",
        "        row_y = []\n",
        "        \n",
        "        for attribute in input_x_label:\n",
        "            # Può essere una singola parola o una frase\n",
        "            element = df_test.at[i, attribute]\n",
        "            element = element.lower()\n",
        "\n",
        "            if len(element.split()) > 1:\n",
        "                tokenized_row = []\n",
        "\n",
        "                for word in element.split():\n",
        "\n",
        "                # Le stop words sono eliminate sono sul lato degli attributi di X\n",
        "                    if word not in stop_words and word in tokenizer.word_index:\n",
        "\n",
        "                         emb_word = tokenizer.word_index[word]\n",
        "                         tokenized_row.append(emb_word)\n",
        "\n",
        "                row_x.extend(tokenized_row)\n",
        "\n",
        "            elif len(element.split()) == 1 and element != \" \"  and element in tokenizer.word_index and element not in stop_words:\n",
        "                    \n",
        "                tokenized_single = tokenizer.word_index[element]\n",
        "                        \n",
        "                row_x.append(tokenized_single)\n",
        "        \n",
        "        tokenized_x.append(row_x)\n",
        "\n",
        "    # Allineamento delle sequenze\n",
        "    padded_x = pad_sequences(tokenized_x, maxlen=max_length_row,\n",
        "                               padding=\"post\", truncating=\"post\")\n",
        "    # Trasformazione in numpy array\n",
        "    padded_x = np.array(padded_x)\n",
        "    x_test = np.reshape(padded_x, [len(padded_x), max_length_row])\n",
        "    \n",
        "    # Predizioni\n",
        "    predictions = model.predict(x_test)\n",
        "\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    predictions = predictions.tolist()\n",
        "\n",
        "    # Mapping y -> parola: classe\n",
        "    # Reverse y -> classe: parola\n",
        "    reverse_mapping = {v:k for k, v in mapping_y.items()}\n",
        "    predictions_to_words = []\n",
        "\n",
        "    # Trasformazione da classi a parole\n",
        "    for p in predictions:\n",
        "      predictions_to_words.append(reverse_mapping.get(p))\n",
        "\n",
        "    predicted_y_label = output_y_label + \"predicted\"\n",
        "\n",
        "    df_test[predicted_y_label] = predictions_to_words\n",
        "    \n",
        "    path_out_dataset = dir_out+\"df_test_result.csv\"\n",
        "    df_test.to_csv(path_out_dataset, sep=\",\", header=0)\n",
        "\n",
        "    return df_test, predicted_y_label\n",
        "\n",
        "\n",
        "def compute_scoring(df, y_label, y_pred_label, mapping_y, dir_out=\"./result/\"):\n",
        "\n",
        "    plot_testing(df[y_label].tolist(), df[y_pred_label].tolist(), mapping_y)\n",
        "    \n",
        "    path_file_out = dir_out+\"result_experiment.txt\"\n",
        "     # Calculate f1 score for true vs predicted values\n",
        "    \n",
        "    with open(path_file_out, \"w+\") as result_file:\n",
        "      \n",
        "      f1 = f1_score(df[y_label], df[y_pred_label], average='weighted')\n",
        "    \n",
        "      print(\"F1-Score Weighted:\", f1, file = result_file)\n",
        "\n",
        "      # Print overall classification report\n",
        "      print(classification_report(df[y_label], df[y_pred_label]), file = result_file)\n",
        "\n",
        "\n",
        "def run(path_df, input_x_label, output_y_label, other_unclassified_values=[]):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # Numero massimo di parole memorizzate nel tokenizer\n",
        "    # param -1 : considera solo quelle più importanti\n",
        "    size_dict_words = 20000\n",
        "    \n",
        "    # Taglia delle sequenze\n",
        "    max_length_row = 100\n",
        "    \n",
        "    # Size dell'embedding\n",
        "    n_features_word = 100\n",
        "\n",
        "    df = pd.read_csv(path_df, sep=\",\", header=0, dtype=\"str\")\n",
        "\n",
        "    is_y_categorical(df, output_y_label)\n",
        "    \n",
        "    plot_dataset(df, output_y_label)\n",
        "\n",
        "    df_train, df_test = random_split(df, split_ratios=[0.8, 0.2])\n",
        "  \n",
        "    df_train.reset_index(drop=True, inplace=True)\n",
        "    df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    df_train = clean_df(df_train, other_unclassified_values=[])\n",
        "    # Considera l'ipotesi che ci siano altri valori semanticamente come null\n",
        "    # es. \"Unclassified\"\n",
        "    \n",
        "    # Tokenizer \n",
        "    tokenizer = fit_tokenizer(df_train, input_x_label, output_y_label,\n",
        "                              size_dict_words, other_unclassified_values=[])\n",
        "    \n",
        "\n",
        "    tokenized_x, y_words = tokenizing_x_y(df_train, input_x_label, output_y_label, tokenizer)\n",
        "\n",
        "    # Allineamento delle sequenze\n",
        "    padded_x = pad_sequences(tokenized_x, maxlen=max_length_row,\n",
        "                               padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    # Mapping Y\n",
        "    # Viene passato il dataset totale e il label di y per mantenere le medesime posizioni nel mapping\n",
        "    # in modo che i grafici abbiano i label nelle medesime del plot del dataset totale\n",
        "    y_train, mapping_y = labeling_y(df_train, output_y_label, y_words)\n",
        "\n",
        "    # Numero di classi del dataset\n",
        "    out_classes = len(mapping_y.keys())\n",
        "\n",
        "    # Trasformazione in numpy array\n",
        "    padded_x = np.array(padded_x)\n",
        "    x_train = np.reshape(padded_x, [len(padded_x), max_length_row])\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "    y_train = np.reshape(y_train, [len(y_train), 1])\n",
        "    \n",
        "    model = train_classifier(x_train, y_train, max_length_row,\n",
        "                             n_features_word, out_classes, size_dict_words)\n",
        "    \n",
        "    df_pred_test, y_label_pred = imputing_missing_values(df_test, input_x_label,\n",
        "                                                         output_y_label, tokenizer, \n",
        "                                                         mapping_y, max_length_row,\n",
        "                                                         n_features_word, model,\n",
        "                                                         other_unclassified_values=None)\n",
        "    \n",
        "    compute_scoring(df_pred_test, output_y_label, y_label_pred, mapping_y)\n",
        "    \n",
        "    end = time.time()\n",
        "    end = end - start\n",
        "    \n",
        "    print(\"Finished in:\", end)\n",
        "    \n",
        "run(path_df=\"./datawig/examples/mae_train_dataset.csv\",\n",
        "    input_x_label = [\"title\",\"text\",\"color\"],\n",
        "    output_y_label = \"finish\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgAFAOCzM9KP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Da simpleimputerintro\n",
        "# WARNING: Correzione errore all'interno del file _hpo.py cambiare datawig.utils -> .utils\n",
        "# Aggiungere il path completo al nome del dataset\n",
        "import datawig.datawig\n",
        "from datawig.datawig import SimpleImputer\n",
        "from datawig.datawig.utils import random_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_testing(y_true, y_pred, mapping_y):\n",
        "    # Plotting dataset\n",
        "\n",
        "    counter_y_true = dict()\n",
        "    \n",
        "    for k in mapping_y.keys():\n",
        "      counter_y_true.update({k:y_true.count(k)}) \n",
        "        \n",
        "    counter_y_pred = dict()\n",
        "\n",
        "    for k in mapping_y.keys():\n",
        "      counter_y_pred.update({k:y_pred.count(k)})\n",
        "\n",
        "    n_classes = len(mapping_y.keys())\n",
        "    \n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.bar(np.arange(0, n_classes), color=\"red\", alpha=0.5, label=\"y_true\", height=counter_y_true.values())\n",
        "    plt.bar(np.arange(0, n_classes), color=\"yellow\", alpha=0.5, label=\"y_pred\", height=counter_y_pred.values())\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.title(\"True vs Pred\")\n",
        "    plt.xticks(np.arange(0, n_classes, step=1.0), labels=mapping_y.keys(), rotation=\"vertical\")\n",
        "    plt.savefig(\"./result_datawig/hist_all_together.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def compute_scoring(df, y_label, y_pred_label, mapping_y, dir_out=\"./result_datawig/\"):\n",
        "\n",
        "    plot_testing(df[y_label].tolist(), df[y_pred_label].tolist(), mapping_y)\n",
        "    \n",
        "    \n",
        "    path_file_out =\"./result_datawig/result_datawig.txt\"\n",
        "     # Calculate f1 score for true vs predicted values\n",
        "    \n",
        "    with open(path_file_out, \"w+\") as result_file:\n",
        "      \n",
        "      f1 = f1_score(df[y_label], df[y_pred_label], average='weighted')\n",
        "    \n",
        "      print(\"F1-Score Weighted:\", f1, file = result_file)\n",
        "\n",
        "      # Print overall classification report\n",
        "      print(classification_report(df[y_label], df[y_pred_label]), file = result_file)\n",
        "\n",
        "def map_y(df, y_label):\n",
        "\n",
        "    values = df[y_label].tolist()\n",
        "    categories = set(y for y in values)\n",
        "    mapping_y = {y:i for i,y in enumerate(categories)}\n",
        "    return mapping_y\n",
        "\n",
        "\"\"\"\n",
        "Load Data\n",
        "\"\"\"\n",
        "input_cols = [\"title\",\"text\",\"color\"]\n",
        "out_col = \"finish\"\n",
        "\n",
        "df = pd.read_csv('./datawig/examples/mae_train_dataset.csv')\n",
        "mapping_y = map_y(df, out_col)\n",
        "df_train, df_test = random_split(df, split_ratios=[0.8, 0.2])\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "Run default SimpleImputer\n",
        "\"\"\"\n",
        "# Initialize a SimpleImputer model\n",
        "imputer = SimpleImputer(\n",
        "    input_columns = input_cols,  # columns containing information about the column we want to impute\n",
        "    output_column = out_col,  # the column we'd like to impute values for\n",
        "    output_path='./result_datawig/imputer_model'  # stores model data and metrics\n",
        ")\n",
        "\n",
        "# Fit an imputer model on the train data\n",
        "imputer.fit(train_df=df_train, num_epochs=3)\n",
        "\n",
        "# Impute missing values and return original dataframe with predictions\n",
        "predictions = imputer.predict(df_test)\n",
        "\n",
        "     # Calculate f1 score for true vs predicted values\n",
        "out_col_pred = out_col + \"_imputed\"\n",
        "\n",
        "compute_scoring(predictions, out_col, out_col_pred, mapping_y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}